{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms for MDPs\n",
    "\n",
    "There are multiple different algorithms for solving MDPs. Some solutions such as value iteration, policy iteration, and linear programming are offline solutions that generate exact results. There are also online solutions computing the results by sampling possible features such as Monte Carlo planning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "\n",
    "When solving an MDP, our ultimate goal is to obtain an optimal policy. We start by looking at Value Iteration and a visualization that should help us understand it better.\n",
    "\n",
    "We start by calculating Value/Utility for each of the states. The Value of each state is the expected sum of discounted future rewards given we start in that state and follow a particular policy $\\pi$. The value or the utility of a state is given by\n",
    "\n",
    "$$U(s)=R(s)+\\gamma\\max_{a\\epsilon A(s)}\\sum_{s'} P(s'\\ |\\ s,a)U(s')$$\n",
    "\n",
    "This is called the Bellman equation. The algorithm Value Iteration (**Fig. 16.2** in the book) relies on finding solutions to this Equation. The intuition Value Iteration works are because values propagate through the state space utilizing local updates. This point will we more clear after we encounter the visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path = [os.path.abspath(\"../../\")] + sys.path\n",
    "from mdp4e import *\n",
    "from notebook4e import psource, pseudocode, plot_pomdp_utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psource(value_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes as inputs two parameters, an MDP to solve and epsilon, the maximum error allowed in the utility of any state. It returns a dictionary containing utilities where the keys are the states and values represent utilities. <br> Value Iteration starts with arbitrary initial values for the utilities, calculates the right side of the Bellman equation and plugs it into the left-hand side, thereby updating the utility of each state from the utilities of its neighbors. \n",
    "This is repeated until equilibrium is reached. \n",
    "It works on the principle of _Dynamic Programming_ - using precomputed information to simplify the subsequent computation. \n",
    "If $U_i(s)$ is the utility value for state $s$ at the $i$ th iteration, the iteration step, called Bellman update, looks like this:\n",
    "\n",
    "$$ U_{i+1}(s) \\leftarrow R(s) + \\gamma \\max_{a \\epsilon A(s)} \\sum_{s'} P(s'\\ |\\ s,a)U_{i}(s') $$\n",
    "\n",
    "As you might have noticed, `value_iteration` has an infinite loop. How do we decide when to stop iterating? \n",
    "The concept of _contraction_ successfully explains the convergence of value iteration. \n",
    "Refer to **Section 17.2.3** of the book for a detailed explanation. \n",
    "In the algorithm, we calculate a value $delta$ that measures the difference in the utilities of the current time step and the previous time step. \n",
    "\n",
    "$$\\delta = \\max{(\\delta, \\begin{vmatrix}U_{i + 1}(s) - U_i(s)\\end{vmatrix})}$$\n",
    "\n",
    "This value of delta decreases as the values of $U_i$ converge.\n",
    "We terminate the algorithm if the $\\delta$ value is less than a threshold value determined by the hyperparameter _epsilon_.\n",
    "\n",
    "$$\\delta \\lt \\epsilon \\frac{(1 - \\gamma)}{\\gamma}$$\n",
    "\n",
    "To summarize, the Bellman update is a _contraction_ by a factor of $gamma$ on the space of utility vectors. \n",
    "Hence, from the properties of contractions in general, it follows that `value_iteration` always converges to a unique solution of the Bellman equations whenever $gamma$ is less than 1.\n",
    "We then terminate the algorithm when a reasonable approximation is achieved.\n",
    "In practice, it often occurs that the policy $pi$ becomes optimal long before the utility function converges. For the given 4 x 3 environment with $gamma = 0.9$, the policy $pi$ is optimal when $i = 4$ (at the 4th iteration), even though the maximum error in the utility function is still 0.46. This can be clarified from **figure 17.6** in the book. Hence, to increase computational efficiency, we often use another method to solve MDPs called Policy Iteration which we will see in the latter part of this notebook. \n",
    "<br>For now, let us solve the **sequential_decision_environment** GridMDP using `value_iteration`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 1): 0.3984432178350046,\n",
       " (1, 2): 0.649585681261095,\n",
       " (3, 2): 1,\n",
       " (0, 0): 0.2962883154554812,\n",
       " (3, 0): 0.12987274656746337,\n",
       " (3, 1): -1,\n",
       " (2, 1): 0.48644001739269643,\n",
       " (2, 0): 0.34475423001241573,\n",
       " (2, 2): 0.7953620878466678,\n",
       " (1, 0): 0.253866998464795,\n",
       " (0, 2): 0.5093943765842497}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_iteration(sequential_decision_environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the pseudocode for the algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudocode(\"Value-Iteration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "To illustrate that values propagate out of states let us create a simple visualization. We will be using a modified version of the value_iteration function which will store U over time. We will also remove the parameter epsilon and instead add the number of iterations we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration_instru(mdp, iterations=20):\n",
    "    U_over_time = []\n",
    "    U1 = {s: 0 for s in mdp.states}\n",
    "    R, T, gamma = mdp.R, mdp.T, mdp.gamma\n",
    "    for _ in range(iterations):\n",
    "        U = U1.copy()\n",
    "        for s in mdp.states:\n",
    "            U1[s] = max(q_value(mdp, s, a, U) for a in mdp.actions(s))\n",
    "        U_over_time.append(U)\n",
    "    return U_over_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a function to create the visualization from the utilities returned by **value_iteration_instru**. The reader need not concern himself with the code that immediately follows as it is the usage of Matplotib with IPython Widgets. If you are interested in reading more about these visit [ipywidgets.readthedocs.io](http://ipywidgets.readthedocs.io)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = 4\n",
    "rows = 3\n",
    "U_over_time = value_iteration_instru(sequential_decision_environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from notebook import make_plot_grid_step_function\n",
    "\n",
    "plot_grid_step = make_plot_grid_step_function(columns, rows, U_over_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d98fa86f0b764eef840e3d433f113579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='iteration', max=15, min=1), Output()), _dom_classes=('wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aaadb0f71fd469d96f460a422bc48fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(ToggleButton(value=False, description='Visualize'), ToggleButtons(description='Extra Del…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from notebook import make_visualize\n",
    "\n",
    "iteration_slider = widgets.IntSlider(min=1, max=15, step=1, value=0)\n",
    "w=widgets.interactive(plot_grid_step,iteration=iteration_slider)\n",
    "display(w)\n",
    "\n",
    "visualize_callback = make_visualize(iteration_slider)\n",
    "\n",
    "visualize_button = widgets.ToggleButton(description = \"Visualize\", value = False)\n",
    "time_select = widgets.ToggleButtons(description='Extra Delay:',options=['0', '0.1', '0.2', '0.5', '0.7', '1.0'])\n",
    "a = widgets.interactive(visualize_callback, Visualize = visualize_button, time_step=time_select)\n",
    "display(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move the slider above to observe how the utility changes across iterations. It is also possible to move the slider using arrow keys or to jump to the value by directly editing the number with a double click. The **Visualize Button** will automatically animate the slider for you. The **Extra Delay Box** allows you to set time delay in seconds up to one second for each time step. There is also an interactive editor for grid-world problems `grid_mdp.py` in the GUI folder for you to play around with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration\n",
    "\n",
    "We have already seen that value iteration converges to the optimal policy long before it accurately estimates the utility function. \n",
    "If one action is clearly better than all the others, then the exact magnitude of the utilities in the states involved need not be precise. \n",
    "The policy iteration algorithm works on this insight. \n",
    "The algorithm executes two fundamental steps:\n",
    "* **Policy evaluation**: Given a policy _&#960;&#7522;_, calculate _U&#7522; = U(&#960;&#7522;)_, the utility of each state if _&#960;&#7522;_ were to be executed.\n",
    "* **Policy improvement**: Calculate a new policy _&#960;&#7522;&#8330;&#8321;_ using one-step look-ahead based on the utility values calculated.\n",
    "\n",
    "The algorithm terminates when the policy improvement step yields no change in the utilities. \n",
    "We now have a simplified version of the Bellman equation\n",
    "\n",
    "$$U_i(s) = R(s) + \\gamma \\sum_{s'}P(s'\\ |\\ s, \\pi_i(s))U_i(s')$$\n",
    "\n",
    "An important observation in this equation is that this equation doesn't have the `max` operator, which makes it linear.\n",
    "For _n_ states, we have _n_ linear equations with _n_ unknowns, which can be solved exactly in time _**O(n&#179;)**_.\n",
    "For more implementational details, have a look at **Section 16.2**.\n",
    "Let us now look at how the expected utility is found and how `policy_iteration` is implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psource(expected_utility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psource(policy_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>Fortunately, it is not necessary to do _exact_ policy evaluation. \n",
    "The utilities can instead be reasonably approximated by performing some number of simplified value iteration steps.\n",
    "The simplified Bellman update equation for the process is\n",
    "\n",
    "$$U_{i+1}(s) \\leftarrow R(s) + \\gamma\\sum_{s'}P(s'\\ |\\ s,\\pi_i(s))U_{i}(s')$$\n",
    "\n",
    "and this is repeated _k_ times to produce the next utility estimate. This is called _modified policy iteration_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psource(policy_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 1): (0, 1),\n",
       " (1, 2): (1, 0),\n",
       " (3, 2): None,\n",
       " (0, 0): (0, 1),\n",
       " (3, 0): (-1, 0),\n",
       " (3, 1): None,\n",
       " (2, 1): (0, 1),\n",
       " (2, 0): (0, 1),\n",
       " (2, 2): (1, 0),\n",
       " (1, 0): (1, 0),\n",
       " (0, 2): (1, 0)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_iteration(sequential_decision_environment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
