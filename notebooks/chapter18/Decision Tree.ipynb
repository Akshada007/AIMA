{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path = [os.path.abspath(\"../../\")] + sys.path\n",
    "from learning4e import *\n",
    "from notebook4e import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DECISION TREE LEARNER\n",
    "\n",
    "### Overview\n",
    "\n",
    "#### Decision Trees\n",
    "A decision tree is a flowchart that uses a tree of decisions and their possible consequences for classification. At each non-leaf node of the tree an attribute of the input is tested, based on which corresponding branch leading to a child-node is selected. At the leaf node the input is classified based on the class label of this leaf node. The paths from root to leaves represent classification rules based on which leaf nodes are assigned class labels.\n",
    "![perceptron](images/decisiontree_fruit.jpg)\n",
    "#### Decision Tree Learning\n",
    "Decision tree learning is the construction of a decision tree from class-labeled training data. The data is expected to be a tuple in which each record of the tuple is an attribute used for classification. The decision tree is built top-down, by choosing a variable at each step that best splits the set of items. There are different metrics for measuring the \"best split\". These generally measure the homogeneity of the target variable within the subsets.\n",
    "\n",
    "#### Gini Impurity\n",
    "Gini impurity of a set is the probability of a randomly chosen element to be incorrectly labeled if it was randomly labeled according to the distribution of labels in the set.\n",
    "\n",
    "$$I_G(p) = \\sum{p_i(1 - p_i)} = 1 - \\sum{p_i^2}$$\n",
    "\n",
    "We select a split which minimizes the Gini impurity in child nodes.\n",
    "\n",
    "#### Information Gain\n",
    "Information gain is based on the concept of entropy from information theory. Entropy is defined as:\n",
    "\n",
    "$$H(p) = -\\sum{p_i \\log_2{p_i}}$$\n",
    "\n",
    "Information Gain is difference between entropy of the parent and weighted sum of entropy of children. The feature used for splitting is the one which provides the most information gain.\n",
    "\n",
    "#### Pseudocode\n",
    "\n",
    "You can view the pseudocode by running the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudocode(\"Decision Tree Learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "The nodes of the tree constructed by our learning algorithm are stored using either `DecisionFork` or `DecisionLeaf` based on whether they are a parent node or a leaf node respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psource(DecisionFork)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DecisionFork holds the attribute, which is tested at that node, and a dict of branches. The branches store the child nodes, one for each of the attribute's values. Calling an object of this class as a function with input tuple as an argument returns the next node in the classification path based on the result of the attribute test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psource(DecisionTreeLearner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation of DecisionTreeLearner provided in learning.py uses information gain as the metric for selecting which attribute to test for splitting. The function builds the tree top-down in a recursive manner. Based on the input it makes one of the four choices:\n",
    "\n",
    "<ol>\n",
    "<li>If the input at the current step has no training data we return the mode of classes of input data received in the parent step (previous level of recursion).</li>\n",
    "<li>If all values in training data belong to the same class it returns a `DecisionLeaf` whose class label is the class which all the data belongs to.</li>\n",
    "<li>If the data has no attributes that can be tested we return the class with highest plurality value in the training data.</li>\n",
    "<li>We choose the attribute which gives the highest amount of entropy gain and return a `DecisionFork` which splits based on this attribute. Each branch recursively calls `decision_tree_learning` to construct the sub-tree.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Here we will apply the Decision Tree Learner to classify our previous example datasets. `DecisionTreeLearner` only takes a `DataSet` object as input and will automatically learn the attributes and targets of data. The learner function will return a `DecisionFork` object which can be use to predict further data example.\n",
    "\n",
    "Decision Tree can be used to simulate the process of human making a decision. It can be applied to either binary or multiple classification. For many complex problems like digit image classification, decision tree can yeild very good results. Thus it can be the first algorithm to in order to investigate a dataset at the beginning of a project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iris Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try on the algorithm on iris example as it has less attributes which makes it clearer to demonstrate. \n",
    "The first step is to load dataset to decision tree learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = DataSet(name=\"iris\")\n",
    "\n",
    "DTL = DecisionTreeLearner(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can try a new example on the trained learner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setosa\n"
     ]
    }
   ],
   "source": [
    "print(DTL([5.1, 3.0, 1.1, 0.1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the Decision Tree learner classifies the sample as \"setosa\" as seen in the previous section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restaurant Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try the decision tree algorithm on Figure 18.3 example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant = DataSet(name=\"restaurant\", attrnames='Alternate Bar Fri/Sat Hungry Patrons Price ' +\n",
    "                   'Raining Reservation Type WaitEstimate Wait')\n",
    "RT = DecisionTreeLearner(restaurant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to print the object, we can see the trained decision tree in the from of nested decision forks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionFork(4, 'Patrons', {'Full': DecisionFork(9, 'WaitEstimate', {'10-30': DecisionFork(8, 'Type', {'Italian': 'No', 'French': 'No', 'Burger': 'Yes', 'Thai': 'Yes'}), '>60': 'No', '0-10': 'No', '30-60': DecisionFork(8, 'Type', {'Italian': 'No', 'French': 'Yes', 'Burger': 'Yes', 'Thai': 'No'})}), 'None': 'No', 'Some': 'Yes'})\n"
     ]
    }
   ],
   "source": [
    "print(RT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try a new example according to the attribute names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alternate', 'Bar', 'Fri/Sat', 'Hungry', 'Patrons', 'Price', 'Raining', 'Reservation', 'Type', 'WaitEstimate', 'Wait']\n"
     ]
    }
   ],
   "source": [
    "print(restaurant.attrnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try some Thai food with full patron and waiting time between 10 and 30 minites:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes\n"
     ]
    }
   ],
   "source": [
    "example = ['Yes', 'No', 'Yes', 'Yes', 'Full', '$', 'No', 'No', 'Thai', '10-30', 'Yes']\n",
    "print(RT(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to try some else like French:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No\n"
     ]
    }
   ],
   "source": [
    "example = ['Yes', 'No', 'Yes', 'Yes', 'Full', '$', 'No', 'No', 'French', '10-30', 'Yes']\n",
    "print(RT(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Overview\n",
    "\n",
    "![random_forest.png](images/random_forest.png)   \n",
    "Image via [src](https://cdn-images-1.medium.com/max/800/0*tG-IWcxL1jg7RkT0.png)\n",
    "\n",
    "#### Random Forest\n",
    "\n",
    "As the name of the algorithm and image above suggest, this algorithm creates the forest with a number of trees. The more number of trees makes the forest robust. In the same way in random forest algorithm, the higher the number of trees in the forest, the higher is the accuray result. The main difference between Random Forest and Decision trees is that, finding the root node and splitting the feature nodes will be random.  \n",
    "\n",
    "Let's see how Rnadom Forest Algorithm work :   \n",
    "Random Forest Algorithm works in two steps, first is the creation of random forest and then the prediction. Let's first see the creation :  \n",
    "\n",
    "The first step in creation is to randomly select 'm' features out of total 'n' features. From these 'm' features calculate the node d using the best split point and then split the node into further nodes using best split. Repeat these steps until 'i' number of nodes are reached. Repeat the entire whole process to build the forest.  \n",
    "\n",
    "Now, let's see how the prediction works\n",
    "Take the test features and predict the outcome for each randomly created decision tree. Calculate the votes for each prediction and the prediction which gets the highest votes would be the final prediction.\n",
    "\n",
    "\n",
    "### Implementation\n",
    "\n",
    "Below mentioned is the implementation of Random Forest Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psource(RandomForest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm creates an ensemble of decision trees using bagging and feature bagging. It takes 'm' examples randomly from the total number of examples and then perform feature bagging with probability p to retain an attribute. All the predictors are predicted from the DecisionTreeLearner and then a final prediction is made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's load MNIST data to a `DataSet` object with predefined load function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img, train_lbl, test_img, test_lbl = load_MNIST(path=\"../../aima-data/MNIST/Digits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use `load_MNIST` correctly, you need to input the correct data directory of MNIST data. Then we compose the training data and labels to a full example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "examples = [np.append(train_img[i], train_lbl[i]) for i in range(len(train_img))]\n",
    "print(len(examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 60000 examples in total, we use the first 100 examples to roughly train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_dataset = DataSet(examples=examples[:100])\n",
    "MNIST_RF = RandomForest(MNIST_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a example in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAK4AAACuCAYAAACvDDbuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAFGklEQVR4nO3dvyt9cRzH8c+Vya/k94SFQTIYWKQrKaPJoO5qkPwoK5mky2Qw2JSMBoPFoEQoE4v8KBmQMokkut8/4PM+Ou659369judjfHfuuZ/h6VPnHI5EJpPJOEBM0f9eAJANwoUkwoUkwoUkwoUkwoUkwoUkwoUkwoWk4rAHJhKJfK4DcM45F/ZBLjsuJBEuJBEuJBEuJBEuJBEuJBEuJBEuJBEuJBEuJBEuJBEuJBEuJBEuJBEuJBEuJBEuJBEuJBEuJBEuJBEuJBEuJBEuJBEuJBEuJBEuJBEuJBEuJBEuJBEuJBEuJBEuJIV+sfNfU19fb8739/e9WUlJiXlsMpn0Zjc3N5HWFWR6etqcz8/Pe7P19XXz2ImJiVwuKa/YcSGJcCGJcCGJcCGJizPnXFGR//ObSqXMY1taWrzZ9fW1eezX11e0hQXo6uryZrOzs+ax5eXl3mxhYSHnayo0dlxIIlxIIlxIIlxIIlxI4q6Cc25sbMybpdPp0J+fnJw057e3t9kuyTnnXEdHhznf2dnxZpWVleaxx8fH3uzl5SXSun4DdlxIIlxIIlxIIlxI+lMXZ01NTeZ8fHw89Dk+Pz+92cfHR9Zr+k7QuqqqqkKfw7o4e319zXpNvwU7LiQRLiQRLiQRLiQRLiQlMplMJtSBiUS+15JTxcX+DZONjQ3z2OHh4dDn3dvb82b9/f3hFxagt7fXm21tbZnH/uSuQnNzsze7u7sL/flCC5kjOy40ES4kES4kES4kxfaRb21trTf7yUVY0GPcxcXFrNfknHOlpaXmfGVlxZv95CJse3vbnD8/P4c+hxJ2XEgiXEgiXEgiXEgiXEiK7V2Furq6SJ+/uroy57u7u6HPYd1BGBkZMY8N+ovesGZmZsx5HH5p3MKOC0mEC0mEC0mEC0mxvTgbHR2N9PnGxkZzHvQ7spaamhpv1tPTk/WavtPe3m7Ord+9zddfJRcSOy4kES4kES4kES4kES4kxfauwv39faTPW/9myTnnhoaGIp03X4Ludqj9lW9Y7LiQRLiQRLiQRLiQFNtXMFVXV3uz8/Nz89iGhoZ8L+e/Ubs44xVMiDXChSTChSTChSTChaTYPvK13pmVTCbNY9va2ryZ9f99nbMfBXd3d/9scSGdnJyYc+t/8a6urprHPj095XRNvwU7LiQRLiQRLiQRLiTF9pFvvnR2dnqz09PTyOc9PDz0ZoODg+axcX2tknM88kXMES4kES4kES4kES4kxfaRb1QVFRXmfG5uLi/ft7y87M3ifPcgKnZcSCJcSCJcSCJcSOKRb4CgFyWfnZ1FOu/R0ZE5HxgY8GZvb2+RvksRj3wRa4QLSYQLSYQLSYQLSTzydfbj3XQ6nZfvWlpaMud/8Q5CFOy4kES4kES4kES4kMQjX+dca2urN7u4uIh8XusVSn19feax7+/vkb8vDnjki1gjXEgiXEgiXEgiXEjika9zbmpqKi/ntd4Hxt2D3GDHhSTChSTChSTChSQuzpxzZWVlkT7/+PhoztfW1iKdF8HYcSGJcCGJcCGJcCGJcCGJuwo5sLm5ac4vLy8LvJK/gx0XkggXkggXkggXkrg4c84dHBx4s1QqZR778PDgzXi0W3jsuJBEuJBEuJBEuJBEuJDEu8Pwq/DuMMQa4UIS4UIS4UIS4UIS4UIS4UIS4UIS4UIS4UJS6F8kD/soDigEdlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxI+gegNfU4U/K/dQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(test_img[-3].reshape((28, 28)))\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction of our random forest model is as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 4, 4, 1, 9]\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(MNIST_RF(test_img[-3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is the same as our own perception of the figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision List Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision list is an algorithm that applys PAC learning to a new hypothesis space. PAC is short for probably approximately correct which means any hypothesis that is consistent with a sufﬁciently large set of training examples is unlikely to be seriously wrong. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
